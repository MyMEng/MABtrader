@book{white2012bandit,
  title={Bandit algorithms for website optimization},
  author={White, John},
  year={2012},
  publisher={" O'Reilly Media, Inc."}
}

@Book{berry+firstedt,
 author    = "D. A. {Berry} and B. {Fristedt}",
 title     = "Bandits Problems
        Sequential Allocation of Experiments. --- 
        (Monographs on statistics and applied probability)",
 publisher = "Chapman and Hall",
 year      =  {1985},
 address   = "11 New Fetter Lane, London EC4P 4EE",
 isbn    = "0 412 24810 7"
}

@Book{gittins+glazebrook+weber,
 author    = "John {Gittins}, Kevin {Glazebrook} and Richard {Weber}",
 title     = "MULTI-ARMED BANDIT ALLOCATION INDICES",
 publisher = "John Willey \& Sons",
 year      =  {2011},
 address   = "The Atrium, Southern Gate, Chichester, West Sussex, PO19 8SQ, United Kingdom",
 isbn    = "978-0-470-67002-6",
 edition   = "2nd"
}

@inproceedings{AYPSze12,
  Abstract = {We introduce a novel technique, which we call online-to-confidence-set conversion. The technique allows us to construct high-probability confidence sets for linear prediction with correlated inputs given the predictions of any algorithm (e.g., online LASSO, exponentiated gradient algorithm, online least-squares, p-norm algorithm) targeting online learning with linear predictors and the quadratic loss. By construction, the size of the confidence set is directly governed by the regret of the online learning algorithm. Constructing tight confidence sets is interesting on its own, but the new technique is given extra weight by the fact having access tight confidence sets underlies a number of important problems. The advantage of our construction here is that progress in constructing better algorithms for online prediction problems directly translates into tighter confidence sets. In this paper, this is demonstrated in the case of linear stochastic bandits. In particular, we introduce the sparse variant of linear stochastic bandits and show that a recent online algorithm together with our online-to-confidence-set conversion allows one to derive algorithms that can exploit if the reward is a function of a sparse linear combination of the components of the chosen action.},
  Author = {Abbasi-Yadkori, Y. and P{\'a}l, D. and Szepesv{\'a}ri, {Cs}.},
  Bibsource = {DBLP, http://dblp.uni-trier.de},
  Booktitle = {AISTAT},
  Date = {2012-04},
  Date-Added = {2012-06-03 14:18:30 -0600},
  Date-Modified = {2013-10-20 21:25:41 +0300},
  Ee = {http://jmlr.csail.mit.edu/proceedings/papers/v22/abbasi-yadkori12/abbasi-yadkori12.pdf},
  Keywords = {bandits, stochastic bandits, theory, online learning, linear bandits},
  Pages = {1--9},
  Pdf = {papers/online-to-confidenceset.pdf},
  Title = {Online-to-confidence-set conversions and application to sparse stochastic bandits},
  Year = {2012}
}

@article {ASMB:ASMB874,
author = {Scott, Steven L.},
title = {A modern Bayesian look at the multi-armed bandit},
journal = {Applied Stochastic Models in Business and Industry},
volume = {26},
number = {6},
publisher = {John Wiley & Sons, Ltd.},
issn = {1526-4025},
url = {http://dx.doi.org/10.1002/asmb.874},
doi = {10.1002/asmb.874},
pages = {639--658},
keywords = {probability matching, exploration vs exploitation, sequential design, Bayesian adaptive design},
year = {2010},
abstract = {A multi-armed bandit is an experiment with the goal of accumulating rewards from a payoff distribution with unknown parameters that are to be learned sequentially. This article describes a heuristic for managing multi-armed bandits called randomized probability matching, which randomly allocates observations to arms according the Bayesian posterior probability that each arm is optimal. Advances in Bayesian computation have made randomized probability matching easy to apply to virtually any payoff distribution. This flexibility frees the experimenter to work with payoff distributions that correspond to certain classical experimental designs that have the potential to outperform methods that are ‘optimal’ in simpler contexts. I summarize the relationships between randomized probability matching and several related heuristics that have been used in the reinforcement learning literature. Copyright © 2010 John Wiley & Sons, Ltd.}
}

@inproceedings{Tang:2013:AAF:2505515.2514700,
 author = {Tang, Liang and Rosales, Romer and Singh, Ajit and Agarwal, Deepak},
 title = {Automatic Ad Format Selection via Contextual Bandits},
 booktitle = {Proceedings of the 22Nd ACM International Conference on Conference on Information \&\#38; Knowledge Management},
 series = {CIKM '13},
 year = {2013},
 isbn = {978-1-4503-2263-8},
 location = {San Francisco, California, USA},
 pages = {1587--1594},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/2505515.2514700},
 doi = {10.1145/2505515.2514700},
 acmid = {2514700},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bandit algorithms, exploration/exploitation, layout, machine learning, offline evaluation, online advertising, personalization, recommender systems},
} 

@inproceedings{graepel2010web,
  title={Web-scale bayesian click-through rate prediction for sponsored search advertising in microsoft's bing search engine},
  author={Graepel, Thore and Candela, Joaquin Q and Borchert, Thomas and Herbrich, Ralf},
  booktitle={Proceedings of the 27th International Conference on Machine Learning (ICML-10)},
  pages={13--20},
  year={2010}
}

@inproceedings{Li:2010:CAP:1772690.1772758,
 author = {Li, Lihong and Chu, Wei and Langford, John and Schapire, Robert E.},
 title = {A Contextual-bandit Approach to Personalized News Article Recommendation},
 booktitle = {Proceedings of the 19th International Conference on World Wide Web},
 series = {WWW '10},
 year = {2010},
 isbn = {978-1-60558-799-8},
 location = {Raleigh, North Carolina, USA},
 pages = {661--670},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1772690.1772758},
 doi = {10.1145/1772690.1772758},
 acmid = {1772758},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {contextual bandit, exploration/exploitation dilemma, personalization, recommender systems, web service},
} 
